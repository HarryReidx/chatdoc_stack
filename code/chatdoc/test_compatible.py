#!/usr/bin/env python3
"""
å…¼å®¹æ€§æµ‹è¯•è„šæœ¬ - å¤„ç†ç‰ˆæœ¬é—®é¢˜
Author: AI Assistant
Date: 2025-07-28
"""
import sys
import os
import time

# è®¾ç½®HuggingFaceé•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

def test_torch_version():
    """æµ‹è¯•PyTorchç‰ˆæœ¬å…¼å®¹æ€§"""
    print("=== æµ‹è¯•PyTorchç‰ˆæœ¬å…¼å®¹æ€§ ===")
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"   CPUçº¿ç¨‹æ•°: {torch.get_num_threads()}")
        
        # æ£€æŸ¥ç‰ˆæœ¬
        version = torch.__version__
        major, minor = map(int, version.split('.')[:2])
        
        if major >= 2 and minor >= 1:
            print("âœ… PyTorchç‰ˆæœ¬æ»¡è¶³è¦æ±‚ (>=2.1)")
            return True
        else:
            print(f"âš ï¸ PyTorchç‰ˆæœ¬è¿‡ä½ ({version})ï¼Œéœ€è¦ >=2.1")
            print("   è¯·è¿è¡Œ: python fix_pytorch_version.py")
            return False
            
    except Exception as e:
        print(f"âŒ PyTorchæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_transformers_basic():
    """æµ‹è¯•transformersåŸºæœ¬åŠŸèƒ½"""
    print("\n=== æµ‹è¯•transformersåŸºæœ¬åŠŸèƒ½ ===")
    
    try:
        import transformers
        print(f"âœ… transformersç‰ˆæœ¬: {transformers.__version__}")
        
        # æµ‹è¯•åŸºæœ¬å¯¼å…¥
        from transformers import AutoTokenizer
        print("âœ… AutoTokenizer å¯¼å…¥æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ transformersæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_sentence_transformers_compatible():
    """æµ‹è¯•sentence-transformerså…¼å®¹æ€§"""
    print("\n=== æµ‹è¯•sentence-transformerså…¼å®¹æ€§ ===")
    
    try:
        # æ£€æŸ¥PyTorchç‰ˆæœ¬
        import torch
        version = torch.__version__
        major, minor = map(int, version.split('.')[:2])
        
        if major < 2 or (major == 2 and minor < 1):
            print(f"âš ï¸ PyTorchç‰ˆæœ¬ {version} å¯èƒ½ä¸å…¼å®¹")
            print("   å°è¯•ä½¿ç”¨å…¼å®¹æ¨¡å¼...")
            
            # å°è¯•ä½¿ç”¨transformersç›´æ¥å®ç°
            return test_manual_embedding()
        
        # æ­£å¸¸æµ‹è¯•sentence-transformers
        from sentence_transformers import SentenceTransformer
        print("âœ… sentence-transformers å¯¼å…¥æˆåŠŸ")
        
        # ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•
        model_name = "paraphrase-multilingual-MiniLM-L12-v2"
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        model = SentenceTransformer(model_name, device='cpu')
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç¼–ç 
        text = "æµ‹è¯•æ–‡æœ¬"
        embedding = model.encode(text)
        print(f"âœ… ç¼–ç æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ sentence-transformersæµ‹è¯•å¤±è´¥: {e}")
        print("   å°è¯•æ‰‹åŠ¨å®ç°...")
        return test_manual_embedding()


def test_manual_embedding():
    """æ‰‹åŠ¨å®ç°åµŒå…¥åŠŸèƒ½ï¼ˆå…¼å®¹æ€§å¤‡é€‰æ–¹æ¡ˆï¼‰"""
    print("\n=== æµ‹è¯•æ‰‹åŠ¨åµŒå…¥å®ç° ===")
    
    try:
        from transformers import AutoTokenizer, AutoModel
        import torch
        
        model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ‰‹åŠ¨å®ç°mean pooling
        def mean_pooling(model_output, attention_mask):
            token_embeddings = model_output[0]
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        
        # æµ‹è¯•ç¼–ç 
        text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')
        
        with torch.no_grad():
            model_output = model(**encoded_input)
            sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
        
        print(f"âœ… æ‰‹åŠ¨åµŒå…¥æˆåŠŸï¼Œç»´åº¦: {sentence_embeddings.shape[1]}")
        return True
        
    except Exception as e:
        print(f"âŒ æ‰‹åŠ¨åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_simple_services():
    """æµ‹è¯•ç®€åŒ–çš„æœåŠ¡å®ç°"""
    print("\n=== æµ‹è¯•ç®€åŒ–æœåŠ¡å®ç° ===")
    
    try:
        # åˆ›å»ºç®€åŒ–çš„åµŒå…¥æœåŠ¡
        class SimpleEmbeddingService:
            def __init__(self):
                from transformers import AutoTokenizer, AutoModel
                import torch
                
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModel.from_pretrained(model_name)
                self.device = 'cpu'
                
            def encode_single(self, text):
                import torch
                encoded_input = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')
                
                with torch.no_grad():
                    model_output = self.model(**encoded_input)
                    # Mean pooling
                    token_embeddings = model_output[0]
                    attention_mask = encoded_input['attention_mask']
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                    sentence_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                
                return sentence_embedding[0].numpy().tolist()
            
            def encode_batch(self, texts):
                return [self.encode_single(text) for text in texts]
        
        print("æ­£åœ¨åˆå§‹åŒ–ç®€åŒ–åµŒå…¥æœåŠ¡...")
        service = SimpleEmbeddingService()
        print("âœ… ç®€åŒ–åµŒå…¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•
        embedding = service.encode_single("æµ‹è¯•æ–‡æœ¬")
        print(f"âœ… å•æ–‡æœ¬åµŒå…¥æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        
        embeddings = service.encode_batch(["æ–‡æœ¬1", "æ–‡æœ¬2"])
        print(f"âœ… æ‰¹é‡åµŒå…¥æˆåŠŸï¼Œæ•°é‡: {len(embeddings)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€åŒ–æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False


def create_fallback_config():
    """åˆ›å»ºå¤‡é€‰é…ç½®"""
    print("\n=== åˆ›å»ºå¤‡é€‰é…ç½® ===")
    
    config_content = """
# å…¼å®¹æ€§é…ç½® - é€‚ç”¨äºç‰ˆæœ¬é—®é¢˜ç¯å¢ƒ
local_services:
  embedding:
    model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    device: "cpu"
    batch_size: 4  # å°æ‰¹æ¬¡ï¼Œé¿å…å†…å­˜é—®é¢˜
    implementation: "manual"  # ä½¿ç”¨æ‰‹åŠ¨å®ç°
  rerank:
    model: "simple"  # ä½¿ç”¨ç®€å•ç›¸ä¼¼åº¦è®¡ç®—
    device: "cpu"
    implementation: "manual"
  pdf_parser:
    engine: "pypdf2"  # ä½¿ç”¨ç¨³å®šçš„PyPDF2
  storage:
    type: "local"
    base_path: "./data/images"

# å…¼å®¹æ€§è®¾ç½®
compatibility:
  use_manual_implementation: true
  fallback_to_simple: true
  max_retries: 3
"""
    
    try:
        with open("config_compatible.yaml", "w", encoding="utf-8") as f:
            f.write(config_content)
        print("âœ… å…¼å®¹æ€§é…ç½®æ–‡ä»¶å·²åˆ›å»º: config_compatible.yaml")
        return True
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("=== å…¼å®¹æ€§æµ‹è¯• ===")
    print("æ­¤æµ‹è¯•å¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜\n")
    
    tests = [
        ("PyTorchç‰ˆæœ¬", test_torch_version),
        ("transformersåŸºæœ¬åŠŸèƒ½", test_transformers_basic),
        ("sentence-transformerså…¼å®¹æ€§", test_sentence_transformers_compatible),
        ("ç®€åŒ–æœåŠ¡å®ç°", test_simple_services),
        ("åˆ›å»ºå¤‡é€‰é…ç½®", create_fallback_config)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*60}")
        print(f"å¼€å§‹æµ‹è¯•: {test_name}")
        print('='*60)
        
        try:
            if test_func():
                passed += 1
                print(f"\nâœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"\nâŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"\nâŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print(f"\n{'='*60}")
    print(f"å…¼å®¹æ€§æµ‹è¯•ç»“æœ: {passed}/{total}")
    print('='*60)
    
    if passed >= 3:
        print("ğŸ‰ åŸºæœ¬å…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ… å¯ä»¥ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")
        print("âœ… å»ºè®®ä½¿ç”¨ config_compatible.yaml é…ç½®")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. å¦‚æœPyTorchç‰ˆæœ¬æœ‰é—®é¢˜ï¼Œè¿è¡Œ: python fix_pytorch_version.py")
        print("2. ä½¿ç”¨å…¼å®¹æ€§é…ç½®å¯åŠ¨chatdoc")
        return True
    else:
        print("âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥")
        print("\nå»ºè®®:")
        print("1. è¿è¡Œ: python fix_pytorch_version.py")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. è€ƒè™‘ä½¿ç”¨æ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
